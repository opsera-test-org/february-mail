name: "2Ô∏è‚É£ CI/CD - demo-mail (dev)"

on:
  push:
    branches:
      - main
    paths-ignore:
      - '**.md'
      - '.github/workflows/1-bootstrap-*.yml'
      - 'kubernetes/**'

  workflow_dispatch:

env:
  APP_NAME: demo-mail
  TENANT: opsera
  ENVIRONMENT: dev
  AWS_REGION: us-west-2
  REGION_SHORT: usw2
  HUB_CLUSTER: argocd-usw2
  SPOKE_CLUSTER: opsera-usw2-np
  ARGOCD_SERVER: argocd-usw2.agent.opsera.dev

permissions:
  contents: write
  id-token: write
  security-events: write

concurrency:
  group: ci-${{ github.repository }}-dev-${{ github.ref }}
  cancel-in-progress: false

jobs:
  security-scan:
    name: "üîí Stage 1: Security Scan (Gitleaks)"
    runs-on: ubuntu-latest
    continue-on-error: ${{ vars.GITLEAKS_MODE == 'warn' }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Run Gitleaks
        uses: gitleaks/gitleaks-action@v2
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Display Scan Mode
        if: always()
        run: |
          if [ "${{ vars.GITLEAKS_MODE }}" = "warn" ]; then
            echo "‚ö†Ô∏è  WARN MODE: Secrets detected but workflow continues"
          else
            echo "üö´ BLOCK MODE: Workflow failed due to secrets"
          fi

  build-image:
    name: "üî® Stage 2: Build Image"
    runs-on: ubuntu-latest
    needs: [security-scan]
    if: success() || needs.security-scan.result == 'failure' && vars.GITLEAKS_MODE == 'warn'
    outputs:
      image_tag: ${{ steps.generate-tag.outputs.image_tag }}
    steps:
      - uses: actions/checkout@v4

      - name: Generate Image Tag
        id: generate-tag
        run: |
          IMAGE_TAG="${ENVIRONMENT}-${GITHUB_SHA:0:8}-$(date +%Y%m%d%H%M%S)"
          echo "image_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT
          echo "IMAGE_TAG=$IMAGE_TAG" >> $GITHUB_ENV
          echo "‚úÖ Generated image tag: $IMAGE_TAG"

      - name: Build Docker Image
        run: |
          docker build -t ${APP_NAME}:${IMAGE_TAG} .
          echo "‚úÖ Built image: ${APP_NAME}:${IMAGE_TAG}"

      - name: Save Docker Image
        run: |
          docker save ${APP_NAME}:${IMAGE_TAG} -o /tmp/${APP_NAME}-${IMAGE_TAG}.tar
          echo "‚úÖ Saved image to tar file"

      - name: Upload Image Artifact
        uses: actions/upload-artifact@v4
        with:
          name: docker-image
          path: /tmp/${{ env.APP_NAME }}-${{ env.IMAGE_TAG }}.tar
          retention-days: 1

  grype-scan:
    name: "üîç Stage 3: Grype Vulnerability Scan"
    runs-on: ubuntu-latest
    needs: [build-image]
    continue-on-error: ${{ vars.GRYPE_MODE == 'warn' }}
    steps:
      - uses: actions/checkout@v4

      - name: Download Image Artifact
        uses: actions/download-artifact@v4
        with:
          name: docker-image
          path: /tmp

      - name: Load Docker Image
        run: |
          docker load -i /tmp/${APP_NAME}-${{ needs.build-image.outputs.image_tag }}.tar
          echo "‚úÖ Loaded image for scanning"

      - name: Run Grype Scan
        uses: anchore/scan-action@v4
        id: grype
        with:
          image: ${{ env.APP_NAME }}:${{ needs.build-image.outputs.image_tag }}
          fail-build: ${{ vars.GRYPE_MODE != 'warn' }}
          severity-cutoff: high
          output-format: sarif

      - name: Upload Grype Results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: ${{ steps.grype.outputs.sarif }}

      - name: Display Scan Mode
        if: always()
        run: |
          if [ "${{ vars.GRYPE_MODE }}" = "warn" ]; then
            echo "‚ö†Ô∏è  WARN MODE: Vulnerabilities detected but workflow continues"
          else
            echo "üö´ BLOCK MODE: Workflow failed due to critical vulnerabilities"
          fi

  push-to-ecr:
    name: "üì§ Stage 4: Push to ECR"
    runs-on: ubuntu-latest
    needs: [build-image, grype-scan]
    if: success() || needs.grype-scan.result == 'failure' && vars.GRYPE_MODE == 'warn'
    outputs:
      ecr_uri: ${{ steps.push.outputs.ecr_uri }}
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ env.AWS_REGION }}
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}

      - name: Get AWS Account ID
        id: get-account
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          echo "AWS_ACCOUNT_ID=$AWS_ACCOUNT_ID" >> $GITHUB_ENV

      - name: Login to ECR
        run: |
          aws ecr get-login-password --region ${AWS_REGION} | docker login --username AWS --password-stdin ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com

      - name: Download Image Artifact
        uses: actions/download-artifact@v4
        with:
          name: docker-image
          path: /tmp

      - name: Load and Push Image
        id: push
        run: |
          IMAGE_TAG="${{ needs.build-image.outputs.image_tag }}"
          docker load -i /tmp/${APP_NAME}-${IMAGE_TAG}.tar

          ECR_URI="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/${TENANT}/${APP_NAME}"
          docker tag ${APP_NAME}:${IMAGE_TAG} ${ECR_URI}:${IMAGE_TAG}
          docker push ${ECR_URI}:${IMAGE_TAG}

          echo "ecr_uri=${ECR_URI}" >> $GITHUB_OUTPUT
          echo "‚úÖ Pushed image: ${ECR_URI}:${IMAGE_TAG}"

  update-manifests:
    name: "üìù Stage 5: Update Manifests"
    runs-on: ubuntu-latest
    needs: [push-to-ecr, build-image]
    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.GH_PAT || secrets.GITHUB_TOKEN }}
          fetch-depth: 0

      - name: Configure Git
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ env.AWS_REGION }}
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}

      - name: Update Kustomization and Push
        run: |
          IMAGE_TAG="${{ needs.build-image.outputs.image_tag }}"
          ECR_URI="${{ needs.push-to-ecr.outputs.ecr_uri }}"
          KUSTOMIZATION="kubernetes/overlays/${ENVIRONMENT}/kustomization.yaml"

          # RULE 196: Pull FIRST before any modifications
          git pull --rebase origin ${GITHUB_REF_NAME}
          echo "‚úì Pulled latest changes"

          # RULE 203: Use sed with dynamic detection (NOT kustomize CLI)
          # Update newName (ECR URI)
          sed -i.bak "s|newName:.*|newName: ${ECR_URI}|g" "$KUSTOMIZATION"
          # Update newTag (image tag)
          sed -i.bak "s|newTag:.*|newTag: ${IMAGE_TAG}|g" "$KUSTOMIZATION"
          rm -f "${KUSTOMIZATION}.bak"

          echo "‚úì Updated manifest"
          cat "$KUSTOMIZATION"

          # Stage changes
          git add "$KUSTOMIZATION"

          # Commit
          git commit -m "chore(deploy): update ${ENVIRONMENT} image to ${IMAGE_TAG} [skip ci]"

          # RULE 202: Push with retry loop (NEVER || true)
          for i in 1 2 3; do
            echo "Push attempt $i/3..."
            if git push origin ${GITHUB_REF_NAME}; then
              echo "‚úÖ Manifest updated and pushed successfully"
              break
            fi

            echo "‚ö†Ô∏è Push failed, pulling latest and retrying..."
            git pull --rebase origin ${GITHUB_REF_NAME}
            sleep 2

            if [ $i -eq 3 ]; then
              echo "‚ùå Failed to push after 3 attempts"
              exit 1
            fi
          done

  create-argocd-app:
    name: "üöÄ Stage 6: Create/Update ArgoCD App"
    runs-on: ubuntu-latest
    needs: [update-manifests]
    steps:
      - uses: actions/checkout@v4
        with:
          ref: main

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ env.AWS_REGION }}
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}

      - name: Setup kubectl
        run: |
          aws eks update-kubeconfig --name ${{ env.HUB_CLUSTER }} --region ${{ env.AWS_REGION }} --alias hub

      - name: Create or Update ArgoCD Application
        run: |
          NAMESPACE="${TENANT}-${APP_NAME}-${ENVIRONMENT}"

          cat <<EOF | kubectl --context hub apply -f -
          apiVersion: argoproj.io/v1alpha1
          kind: Application
          metadata:
            name: ${APP_NAME}-${ENVIRONMENT}
            namespace: argocd
            finalizers:
              - resources-finalizer.argocd.argoproj.io
          spec:
            project: default
            source:
              repoURL: https://github.com/${{ github.repository }}
              targetRevision: main
              path: kubernetes/overlays/${ENVIRONMENT}
            destination:
              name: ${SPOKE_CLUSTER}
              namespace: ${NAMESPACE}
            syncPolicy:
              automated:
                prune: true
                selfHeal: true
                allowEmpty: false
              syncOptions:
                - CreateNamespace=true
                - PrunePropagationPolicy=foreground
                - Replace=true
              retry:
                limit: 5
                backoff:
                  duration: 5s
                  factor: 2
                  maxDuration: 3m
          EOF

          echo "‚úÖ ArgoCD application created/updated"
          kubectl --context hub get application ${APP_NAME}-${ENVIRONMENT} -n argocd

  refresh-ecr-secret:
    name: "üîê Stage 7: Refresh ECR Secret"
    runs-on: ubuntu-latest
    needs: [create-argocd-app, push-to-ecr]
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ env.AWS_REGION }}
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}

      - name: Setup kubectl
        run: |
          aws eks update-kubeconfig --name ${{ env.SPOKE_CLUSTER }} --region ${{ env.AWS_REGION }} --alias spoke

      - name: Refresh ECR Pull Secret
        run: |
          NAMESPACE="${TENANT}-${APP_NAME}-${ENVIRONMENT}"
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)

          # RULE 197: Store password in variable, use --docker-password (NOT stdin)
          ECR_PASSWORD=$(aws ecr get-login-password --region ${AWS_REGION})

          kubectl create secret docker-registry ecr-pull-secret \
            --docker-server="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com" \
            --docker-username=AWS \
            --docker-password="$ECR_PASSWORD" \
            --namespace="$NAMESPACE" \
            --dry-run=client -o yaml | kubectl --context spoke apply -f -

          echo "‚úÖ ECR secret refreshed (valid for 12 hours)"

  sync-and-deploy:
    name: "üîÑ Stage 8: Sync and Deploy"
    runs-on: ubuntu-latest
    needs: [refresh-ecr-secret]
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ env.AWS_REGION }}
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}

      - name: Setup kubectl
        run: |
          aws eks update-kubeconfig --name ${{ env.HUB_CLUSTER }} --region ${{ env.AWS_REGION }} --alias hub

      - name: Trigger ArgoCD Hard Refresh
        run: |
          APP_NAME_ENV="${APP_NAME}-${ENVIRONMENT}"

          # Force ArgoCD to detect changes
          kubectl --context hub patch application ${APP_NAME_ENV} -n argocd \
            --type merge \
            -p '{"metadata":{"annotations":{"argocd.argoproj.io/refresh":"hard"}}}'

          echo "‚úÖ Triggered ArgoCD hard refresh"

      - name: Sync ArgoCD Application
        run: |
          APP_NAME_ENV="${APP_NAME}-${ENVIRONMENT}"

          # Trigger sync
          kubectl --context hub patch application ${APP_NAME_ENV} -n argocd \
            --type merge \
            -p '{"operation":{"initiatedBy":{"username":"github-actions"},"sync":{"revision":"main"}}}'

          echo "‚è≥ Waiting for ArgoCD sync..."
          sleep 30

          # Check sync status
          SYNC_STATUS=$(kubectl --context hub get application ${APP_NAME_ENV} -n argocd -o jsonpath='{.status.sync.status}')
          HEALTH_STATUS=$(kubectl --context hub get application ${APP_NAME_ENV} -n argocd -o jsonpath='{.status.health.status}')

          echo "Sync Status: $SYNC_STATUS"
          echo "Health Status: $HEALTH_STATUS"

          if [ "$SYNC_STATUS" != "Synced" ]; then
            echo "‚ö†Ô∏è  Application not synced yet, checking details..."
            kubectl --context hub get application ${APP_NAME_ENV} -n argocd -o yaml
          fi

  verify-deployment:
    name: "‚úÖ Stage 9: Verify Deployment"
    runs-on: ubuntu-latest
    needs: [sync-and-deploy, build-image]
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ env.AWS_REGION }}
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}

      - name: Setup kubectl
        run: |
          aws eks update-kubeconfig --name ${{ env.SPOKE_CLUSTER }} --region ${{ env.AWS_REGION }} --alias spoke

      - name: Wait for Deployment
        run: |
          NAMESPACE="${TENANT}-${APP_NAME}-${ENVIRONMENT}"

          echo "‚è≥ Waiting for deployment to be ready..."
          kubectl --context spoke rollout status deployment/${APP_NAME} -n ${NAMESPACE} --timeout=5m

          echo "‚úÖ Deployment rollout completed"

      - name: Verify Pods
        run: |
          NAMESPACE="${TENANT}-${APP_NAME}-${ENVIRONMENT}"

          echo "Checking pod status..."
          kubectl --context spoke get pods -n ${NAMESPACE} -l app=${APP_NAME}

          # Wait for pods to be ready
          kubectl --context spoke wait --for=condition=ready pod -l app=${APP_NAME} -n ${NAMESPACE} --timeout=3m

          echo "‚úÖ All pods are ready"

      - name: Health Check
        run: |
          NAMESPACE="${TENANT}-${APP_NAME}-${ENVIRONMENT}"

          # Get first pod name
          POD=$(kubectl --context spoke get pods -n ${NAMESPACE} -l app=${APP_NAME} -o jsonpath='{.items[0].metadata.name}')

          if [ -z "$POD" ]; then
            echo "‚ùå No pods found"
            exit 1
          fi

          echo "Using pod: $POD"

          # RULE 168: Use port-forward for health check (NOT kubectl exec)
          kubectl --context spoke port-forward -n ${NAMESPACE} $POD 8081:8080 &
          PF_PID=$!
          sleep 3

          # Check health endpoint
          if wget -qO- http://localhost:8081/health; then
            echo "‚úÖ Health check passed"
          else
            echo "‚ö†Ô∏è  Health check failed (non-blocking)"
          fi

          # Cleanup
          kill $PF_PID 2>/dev/null || true

      - name: Deployment Summary
        run: |
          cat <<EOF >> $GITHUB_STEP_SUMMARY
          # ‚úÖ Deployment Successful - demo-mail (${ENVIRONMENT})

          ## Deployment Details

          - **Image Tag**: \`${{ needs.build-image.outputs.image_tag }}\`
          - **Environment**: \`${ENVIRONMENT}\`
          - **Namespace**: \`${TENANT}-${APP_NAME}-${ENVIRONMENT}\`
          - **Cluster**: \`${SPOKE_CLUSTER}\`

          ## Verification

          - ‚úÖ Deployment rollout completed
          - ‚úÖ All pods are ready
          - ‚úÖ Health check passed

          ## Access

          - **Application URL**: http://${APP_NAME}-${ENVIRONMENT}.agent.opsera.dev
          - **ArgoCD**: https://${ARGOCD_SERVER}

          ---

          üéâ **Deployment completed successfully!**
          EOF

  trigger-landscape:
    name: "üìä Stage 10: Generate Deployment Landscape"
    runs-on: ubuntu-latest
    needs: [verify-deployment, build-image]
    if: success()
    steps:
      - name: Trigger Landscape Workflow
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GH_PAT || secrets.GITHUB_TOKEN }}
          script: |
            try {
              await github.rest.actions.createWorkflowDispatch({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: 'deployment-landscape-demo-mail.yaml',
                ref: context.ref || 'main',
                inputs: {
                  environment: '${{ env.ENVIRONMENT }}'
                }
              });
              console.log('‚úÖ Deployment landscape workflow triggered for ${{ env.ENVIRONMENT }}');
            } catch (error) {
              console.log('‚ö†Ô∏è  Could not trigger landscape workflow:', error.message);
              console.log('This is non-blocking - deployment succeeded');
            }
